name_node                         = hdfs://analytics-hadoop
job_tracker                       = resourcemanager.analytics.eqiad.wmnet:8032
queue_name                        = default
user = ellery


# Base path in HDFS to record impression oozie files.
# Other files will be used relative to this path.

task = geo_pageviews_daily

job_directory  = ${name_node}/user/ellery/${task}/oozie


oozie_directory  = ${name_node}/wmf/refinery/current/oozie

state_directory  = ${name_node}/user/ellery/${task}/oozie/state
version = _v0_1

# HDFS path to directory where webrequest data is time bucketed.
webrequest_data_directory=${name_node}/wmf/data/wmf/webrequest


workflow_file = ${job_directory}/workflow.xml
oozie.coord.application.path=${job_directory}/coordinator.xml


freq=60
start_time=2015-01-01T00:00Z
stop_time=2050-01-01T06:00Z


hive_script = ${job_directory}/hive_query.sql
hive_site_xml = ${name_node}/wmf/refinery/current/oozie/util/hive/hive-site.xml


oozie.use.system.libpath          = true
oozie.action.external.stats.write = true

hive_task_table_directory = ${name_node}/user/hive/warehouse/${user}.db/${task}