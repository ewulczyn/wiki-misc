{
 "metadata": {
  "name": "",
  "signature": "sha256:0842e4bd1e833958df5704eaed7007daef3743d012b9eee39e3e75db458494b8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from  scipy.stats import binom\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import math \n",
      "import pandas as pd\n",
      "pd.options.display.mpl_style = 'default'\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "P(N=j | n) = \\frac{P(n | N=j)*P(N=j)}{\\sum\\limits_{i=n}^N{P(n|N=i)*P(N=i)}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#AB Testing with Sampled Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Wikimedia Foundation (WMF) is in the unique position that we have a ton of traffic and very little resources. When AB testing banners, we get so many impressions we have to sample the number of impressions we record. This complicates estimating the conversion rate for our banners. In traditional AB testing, we model the conversion rate as a bernoulli random variable. When running an AB test with unsampled impression counts, all the uncertainty in true conversion rates comes from the fact that a conversion is a random event and we have a finite number of impressions. However, if we sample the number of impressions, we also need to deal with the uncertainty in the number of banners we actually served! What follows is an extension the bayesian hypothesis test for bernoulli data that accounts for the additonal uncertainty introduced by sampling.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Why do we sample impressions?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before I dig into the theory, let me explain exactly what I mean when I say the impression numbers are sampled and why they are sampled. During a fundraising campaign, we try to serve a banner on every pageview. When the page loads, it fetches the current banner for the experimental condition the client is in. Depending on client side cookies set by previous fundraising banners, the banner may or may not display. For example, if the client closed a banner in the last two weeks, the banner may choose to hide itself. Recording whether there was an impression requires sending an extra request back to our servers. The operations team deems it unacceptable to send such an banner impression logging request for every pageview. So instead, we send the request back with probability r (0.01).\n",
      "\n",
      "At this point I assume you are scratching your head. Why can't we just decide if the banner will be displayed server side and avoid sending the request. Well, the WMF infrastructure is designed to serve requests for non logged-in users out of cache, even banners. As such it is not possible to evaluate if the banner should be shown based on the client's fundraising cookies server-side. I'm sure there are ways to get around sampling, but the fact that impressons are sampled is a constraint I currenlty face."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Estimating the number of Impressions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first challenge in developing theory for AB testing under these circumstances is to estimate how many impressions there really were (N) given the number of logged impression records (n). To make this formal, consider a random process in which we randomly record an event with fixed, known probability $r$. Say we have $n$ recorded events. Lets work out $P(N | n)$, the distribution over the true number of events $N$, given the sampling rate $r$ and the number of recorded events $n$:\n",
      "\n",
      "Note: If you remember your discrete distributions, this might sound reminiscent of a negative binomial random variable. That is almost correct. The negative binomial random variable corresponds to the number of attempts (N) until the nth success. In our scenario, we don't know how many impressions there where after the last recorded impression (success). What follows is the derivation of the correct distribution. For situations where you have so much data that you have to sample, it turns out to be very similar to the negative binomial. So if you don't enjoy proofs, feel free to skip to the next section.\n",
      "\n",
      "We know that $P(n|N)$ ~  Binomial(N, r). Applying Bayes rule, we get:\n",
      "\n",
      "$$ P(N=j |n) = \\frac{P(n | N=j)*P(N=j)}{\\sum\\limits_{i=n}^N{P(n|N=i)*P(N=i)}}$$\n",
      "\n",
      "If we assume a uniform prior over $N$ (this is a big one), \n",
      "\n",
      "$$ P(N=j |n) = \\frac{P(n | N=j)}{\\sum\\limits_{i=n}^\\infty{P(n|N=i)}} = \\frac{{j \\choose n} r^{n} (1-r)^{j-n}}{\\sum\\limits_{i=n}^\\infty{{i \\choose n} r^{n} (1-r)^{i-n}}} $$\n",
      "\n",
      "Lets work out the infinite sum in the denominator:\n",
      "\n",
      "$$\n",
      "\\sum\\limits_{i=n}^\\infty{{i \\choose n} r^{n} (1-r)^{i-n}} = \n",
      "\\sum\\limits_{i^\\prime=n^\\prime}^\\infty{{i^\\prime -1 \\choose n^\\prime -1} r^{n^\\prime -1} (1-r)^{i^\\prime-n^\\prime}} = \n",
      "\\frac{1}{r} * \\sum\\limits_{i^\\prime=n^\\prime}^\\infty{{i^\\prime -1 \\choose n^\\prime -1} r^{n^\\prime} (1-r)^{i^\\prime-n^\\prime}} = \\frac{1}{r}\n",
      "$$\n",
      "\n",
      "\n",
      "In step 1, we make a transformation of variables. We set $n = n^\\prime -1$ and $i = i^\\prime -1$. In step 2 we make use of the fact that ${i^\\prime -1 \\choose n^\\prime -1} r^{n^\\prime} (1-r)^{i^\\prime-n^\\prime}$ is the pmf of the negative binomial distribution.\n",
      "\n",
      "So, we arrive at our final result:\n",
      "$$\n",
      "P(N | r, n) = \\begin{cases}\n",
      "0, &{\\rm if}\\ N\\ \\text{less than}\\ n  \\\\\n",
      "{N \\choose n} r^{n+1} (1-r)^{N-n}, &{\\rm otherwise}\n",
      "\\end{cases}\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, lets prove that this is a real distribution:\n",
      "$$\n",
      "\\sum\\limits_{i=n}^\\infty{{i \\choose n} r^{n+1} (1-r)^{i-n}}=\n",
      "r * \\sum\\limits_{i=n}^\\infty{{i \\choose n} r^{n} (1-r)^{i-n}} = r * \\frac{1}{r} = 1\n",
      "$$\n",
      "\n",
      "In step 1, we make use of the above proof showing $\\sum\\limits_{i=n}^\\infty{{i \\choose n} r^{n} (1-r)^{i-n}} = \\frac{1}{r}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets refer to this newly derived distribution as the \"inverse binomial distribution\". Next, lets work out the expectation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "E[N] = \n",
      "\\sum\\limits_{i=n}^\\infty{i*{i \\choose n} r^{n+1} (1-r)^{i-n}} = \n",
      "$$\n",
      "$$\n",
      "\\sum\\limits_{i^\\prime=n^\\prime}^\\infty{(i^\\prime -1)*{i^\\prime -1 \\choose n^\\prime -1} r^{n^\\prime} (1-r)^{i^\\prime-n^\\prime}} = \n",
      "$$\n",
      "$$\n",
      "\\sum\\limits_{i^\\prime=n^\\prime}^\\infty{i^\\prime * {i^\\prime -1 \\choose n^\\prime -1} r^{n^\\prime} (1-r)^{i^\\prime-n^\\prime}} - \\sum\\limits_{i^\\prime=n^\\prime}^\\infty{{i^\\prime -1 \\choose n^\\prime -1} r^{n^\\prime} (1-r)^{i^\\prime-n^\\prime}}=\n",
      "$$\n",
      "$$\n",
      "\\frac{n^\\prime}{r} - 1 =\n",
      "$$\n",
      "$$\n",
      "\\frac{n + 1}{r} -1\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What a nice result! In step 3 we make use of the fact that $\\sum\\limits_{i^\\prime=n^\\prime}^\\infty{i^\\prime * {i^\\prime -1 \\choose n^\\prime -1} r^{n^\\prime} (1-r)^{i^\\prime-n^\\prime}}$ is the expectation of a negative binomial distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# simple implementation of n choose k\n",
      "def nCk(n,k):\n",
      "    f = math.factorial\n",
      "    return f(n) / f(k) / f(n-k)\n",
      "\n",
      "# probability mass function for the Number of original data points, given the sampling rate and the number\n",
      "# of observed points\n",
      "def inverse_binom(N, n, r):\n",
      "    return nCk(N, n)*(r**(n+1))*((1-r)**(N-n))\n",
      "\n",
      "def compute_expectation(n, r):\n",
      "    return (n+1)/r -1\n",
      "\n",
      "\n",
      "def is_prob(n, r, limit):\n",
      "    ex = 0.0\n",
      "    for i in range(n, limit):\n",
      "        ex += inverse_binom(i, n, r)\n",
      "    return ex\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Just so you believe me that the inverse binomial is a real distribution\n",
      "print is_prob(100, 0.1, 2000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets look at the expected number of points as a function of the sampling rate, given that we observe 1 example:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare_negative_to_inverse(n):\n",
      "    rs = np.arange(0.1, 0.99, 0.1)\n",
      "    expectations = [compute_expectation(n, r) for r in rs]\n",
      "    plt.plot(rs, expectations, label='inverse binomial (n, r)')\n",
      "    plt.plot(rs, [n/i for i in rs], label = 'negative binomial (n, r)' )\n",
      "    plt.plot(rs, [(n+1)/i for i in rs], label='negative binomial (n+1, r)' )\n",
      "    plt.legend()\n",
      "    plt.xlabel('samplig rate: r')\n",
      "    plt.ylabel('expectation')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def negative_binomial_expectation(n, r):\n",
      "    return n/r\n",
      "\n",
      "def inverse_binomial_expectation(n, r):\n",
      "    return (n+1)/r -1\n",
      "\n",
      "def compare_expectations(n):\n",
      "    fig = plt.figure(figsize =(10, 6))\n",
      "    rs = np.arange(0.1, 0.99, 0.1)\n",
      "    inv_binom = [inverse_binomial_expectation(n, r) for r in rs]\n",
      "    neg_binom_lower = [negative_binomial_expectation(n, r) for r in rs]\n",
      "    neg_binom_upper = [negative_binomial_expectation(n+1, r) for r in rs]\n",
      "    plt.plot(rs, inv_binom, label='inverse binomial n=%d' % n)\n",
      "    plt.plot(rs, neg_binom_lower, label = 'negative binomial n=%d' % n )\n",
      "    plt.plot(rs,neg_binom_upper, label='negative binomial n=%d' % (n+1))\n",
      "    plt.legend()\n",
      "    plt.xlabel('samplig rate: r')\n",
      "    plt.ylabel('expectation')\n",
      "    plt.show()\n",
      "\n",
      "compare_expectations(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compare_negative_to_inverse(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As mentioned above, the negative binomial random variable is the expected number of coin tosses before the nth heads. This is pretty close to our inverse binomial random variable, the difference being that we are not guaranteed that the last of our data points was sampled. There could be arbitrarily many data points that we did not sample after the last one we sampled. Hence we would expect the expectation of the negative binomial distribution to be a lower bound for the inverse binomial distribution. Furthermore the the expectation of the negative binomial distribution, where we act as if we saw one more sample than we really did, is an upper bound for our inverse binomial rv.\n",
      "\n",
      "Depending on r the inverse binomial, swings between it supper and lower bounds. Lets see what happens for larger values on n."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "compare_negative_to_inverse(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If n increase, the difference in expectation between the distributions becomes less and less significant. Hence, for fundraising AB tests, we may as well assume that our impressions is negative binomial."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The Traditional Bayesian AB Test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In traditional bayesian AB testing, we model a conversion as bernoulli random variable. We also model the conversion rate p as a random variable following a beta distribution. Before observing any data, we model p as $beta(a_{prior}, b_{prior})$. Usually, we set $a_{prior}=b_{prior}=1$, which corresponds to a uniform distribution over the interval $[0,1]$. After running a banner we count how many impressions led to a donation (successes) and how many impressions did not (failures).\n",
      "\n",
      "\n",
      "Then we update the distribution over p to be  $beta(a_{prior} + successes, b_{prior} + failures)$. Once we have the distributions for the conversion rate for each of the banners, we can use numerical methods to arrive at the distribution over functions of the conversion rates. We will be most interested in the distribution over the percent difference in conversion rates. For a more thorough introduction to bayesian AB testing, check out (http://engineering.richrelevance.com/bayesian-ab-tests/)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below demonstrates running a traditional bayesian AB test, where the the number of impressions are known in advance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.random import binomial, beta\n",
      "\n",
      "# True donation rates\n",
      "p_A = 0.5\n",
      "p_B = 0.6 # B has a 20% lift over A\n",
      "\n",
      "\n",
      "# True number of impressions per banner\n",
      "N_A = 1000\n",
      "N_B = 1200\n",
      "\n",
      "#simulate Running the banner\n",
      "successes_A = binomial(N_A, p_A)\n",
      "successes_B = binomial(N_B, p_B)\n",
      "\n",
      "# form posterior distribution over the lift  banner B gives over banner A\n",
      "failures_A = N_A -successes_A\n",
      "failures_B = N_B -successes_B\n",
      "\n",
      "N_samples = 20000\n",
      "p_A_posterior = beta(1+successes_A, 1+failures_A, size = N_samples)\n",
      "p_B_posterior =  beta(1+successes_B, 1+failures_B, size = N_samples)\n",
      "\n",
      "lift_distribution = (p_B_posterior-p_A_posterior)/p_A_posterior\n",
      "lift_distribution = lift_distribution *100\n",
      "\n",
      "# 1-a Credible interval for the lift that B ahs over A\n",
      "a = 10\n",
      "lower = np.percentile(lift_distribution, a/2)\n",
      "upper = np.percentile(lift_distribution, 100- a/2)\n",
      "print \"%d%% Confidence Interval for the lift B has over A: (%f, %f)\" %(100-a, lower, upper)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The Traditional Method Break Down with Sampled Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "The method described above won't work for us. When we sample the number of impressions, we still know how many successes we got, since we process the payment for every donation and the donation hits our accounts. But we do not how many impressions did not lead to a donation (failures). We will have to find a way of incorporate the uncertainty we have in the number of failed impressions into the traditional method. \n",
      "\n",
      "Lets start with a naive approach of simply using the expected number of failed impression given the sampling rate r and the number of observed failed impressions n: $\\frac{(n+1)}{r} - 1$. \n",
      "\n",
      "\n",
      "Note: How do we get the number of failed impressions when all the have is the number of observed impressions? We will need to generate an ID on each impression and log that ID with each impression and each donation. Then we can get the number of observed impressions that did not lead to a donation by removing impressions with the same ID as a donation.\n",
      "\n",
      "Lets see how this naive approach behaves. We will be interested in measuring how often our 100-a% confidence intervals for the lift that banner B has over banner A cover the true lift."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.random import negative_binomial, binomial, beta\n",
      "\n",
      "\n",
      "def get_p_dist_naive(successes, observed_failures, r):\n",
      "    \"\"\"\n",
      "    get posterior distribution over the conversion\n",
      "    rate p, when using the expected number of failed impressions\n",
      "    given a sampling rate of r and n observed failures\n",
      "    \"\"\"\n",
      "    estimated_failures = int(compute_expectation(observed_failures, r))\n",
      "    return beta(1+successes, 1+estimated_failures, size = 20000)\n",
      "\n",
      "\n",
      "def get_lift_confidence_interval(p_A_posterior, p_B_posterior, conf):\n",
      "    \"\"\"\n",
      "    compute a credible interval over the lift that\n",
      "    banner A has over banner B given the data\n",
      "    \"\"\"\n",
      "    a = (100-conf) \n",
      "    lift_distribution = (p_B_posterior-p_A_posterior)/p_A_posterior\n",
      "    lift_distribution = lift_distribution *100\n",
      "    lower = np.percentile(lift_distribution, a/2)\n",
      "    upper = np.percentile(lift_distribution, 100- a/2)\n",
      "    return (lower, upper)\n",
      "\n",
      "def lift_interval_covers_true_lift(interval,true_lift):\n",
      "    \"\"\"\n",
      "    Check if the credible interval covers the true parameter\n",
      "    \"\"\"\n",
      "    if interval[1] < true_lift:\n",
      "        return 0\n",
      "    if interval[0] > true_lift:\n",
      "        return 0\n",
      "    return 1\n",
      "        \n",
      "\n",
      "    \n",
      "\n",
      "def measure_coverage(N, p_A, lift, r, get_p_dist, iters = 500, conf = 90, ):\n",
      "    \"\"\"\n",
      "    This function simulates iters AB tests and measures the coverage of the \n",
      "    confidence interval\n",
      "    \"\"\"\n",
      "    p_B = p_A * ((100.+lift)/100.)\n",
      "\n",
      "    interval_covers = 0.0\n",
      "\n",
      "\n",
      "    for i in range(iters):\n",
      "        successes_A = binomial(N, p_A)\n",
      "        observed_failures_A = binomial(N-successes_A, r)\n",
      "        p_A_posterior = get_p_dist(successes_A, observed_failures_A, r)\n",
      "\n",
      "\n",
      "        successes_B = binomial(N, p_B)\n",
      "        observed_failures_B = binomial(N-successes_B, r)\n",
      "        p_B_posterior = get_p_dist(successes_B, observed_failures_B, r)\n",
      "\n",
      "        interval = get_lift_confidence_interval(p_A_posterior, p_B_posterior, conf)\n",
      "        interval_covers += lift_interval_covers_true_lift(interval, lift)\n",
      "    coverage = (interval_covers/iters)*100\n",
      "    print \"%d %% lift interval covers true lift covers %f%% of the time\" % (conf, coverage)\n",
      "    return coverage\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets see what happens when we don't sample. This is equivalent to the traditional method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 1000000\n",
      "p_A = 0.008\n",
      "lift = 3.\n",
      "rs = [1.0, 1/10.,1/50., 1/100.,1/500., 1/1000.]\n",
      "naive_coverages = [measure_coverage(N, p_A, lift, r, get_p_dist_naive, iters = 1000) for r in rs]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = range(len(rs))\n",
      "\n",
      "fig = plt.figure(figsize =(10, 6))\n",
      "plt.plot(x, naive_coverages)\n",
      "plt.xlabel('samplig rate: r')\n",
      "plt.xticks (range(len(x)), [\"1\", \"1/10\", \"1/50\", \"1/100\", \"1/500\", \"1/1000\"])\n",
      "plt.ylabel('percent coverage')\n",
      "plt.title(\"Coverage of 90% CIs as a function of sampling rate\")\n",
      "plt.ylim(0, 100)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "proper_coverages = [measure_coverage(N, p_A, lift, r, get_p_dist_proper, iters = 1000) for r in rs]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = range(len(rs))\n",
      "\n",
      "fig = plt.figure(figsize =(10, 6))\n",
      "plt.plot(x, proper_coverages)\n",
      "plt.xlabel('samplig rate: r')\n",
      "plt.xticks (range(len(x)), [\"1\", \"1/10\", \"1/50\", \"1/100\", \"1/500\", \"1/1000\"])\n",
      "plt.ylabel('percent coverage')\n",
      "plt.title(\"Coverage of 90% CIs as a function of sampling rate\")\n",
      "plt.ylim(0, 100)\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great, we expect the traditonal method to give good confidence intervals. Lets wee what happens as we start sampling more heavily"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = 1/10.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_naive, iters = 1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = 1/50.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_naive, iters = 1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = 1/100.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_naive, iters = 1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = 1/500.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_naive, iters = 1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = 1/1000.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_naive, iters = 1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see the traditonal method gets worse and worse as we increase the sampling rate. Lets incormorate the uncertainty we have in the number of failed impressions, first sampling from the inverse binomial to get the number of failed impressions and them sampling from the beta:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_p_dist_proper(successes, observed_failures, r):\n",
      "    \"\"\"\n",
      "    get posterior distribution over the conversion\n",
      "    rate p, in stead of using a fixed estimate\n",
      "    for the number of failed impressions, given a sampling rate of r and n observed failures\n",
      "    lets sample from a negative binomial\n",
      "    (this should be an inverse binomial)\n",
      "    \"\"\"\n",
      "    missing_failures_distribution = negative_binomial(observed_failures, r, size = 20000)\n",
      "    failures_distribution = missing_failures_distribution + observed_failures\n",
      "    def sample(num_failures):\n",
      "        return beta(1+successes, 1+num_failures)\n",
      "    p_hat = map(sample, failures_distribution)\n",
      "    return np.array(p_hat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 1000000\n",
      "p_A = 0.008\n",
      "lift = 3.\n",
      "r = 1 / 1.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_proper, iters = 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sanity Check Passed: Without sampling this method reduces to the traditional test.\n",
      "        "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = 1/10.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_proper, iters = 1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = 1/50.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_proper, iters = 1000)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = 1/100.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_proper, iters = 1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = 1/500.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_proper, iters = 1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = 1/1000.\n",
      "measure_coverage(N, p_A, lift, r, get_p_dist_proper, iters = 1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next steps:\n",
      "\u00df- figure out how much longer we need to run our average test as a function of sampling rate"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def prob_A_is_better(dA, dB):\n",
      "    return (dA > dB).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}