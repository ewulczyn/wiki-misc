{
 "metadata": {
  "name": "",
  "signature": "sha256:ec74022013c5a64e25fdcd75f2eb04582a8570358f2412a3b04aad5f3d1135ba"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#ClickStream - Getting Started"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This post gives an introduction to working with the newly released [Wikipedia Clickstream](http://figshare.com/articles/Wikipedia_Clickstream/1305770) dataset. At a high level, it shows how people get to a Wikipedia article and what articles they click on next. Said another way, it gives a weighted network of articles, where each edge weight corresponds to how often people navigate from one page to another. To give an example, consider the figure below, which shows incoming and outgoing traffic to the \"London\" article."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(filename='../images/London_Sankey.png', width=800, height=500)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The example shows that most people found the \"London\" page through Google Search and that only a small fraction of readers went on to another article. Before diving into some examples of working with the data, let me give a more detailed explanation of how the data was collected. Following that, I will give a brief tour of the data before doing a sample community detection analysis."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Data Preparation\n",
      "\n",
      "The data contains counts of `(referer, request)` pairs extracted from the request logs of English Wikipedia. When a client requests a resource by following a link or performing a search, the URI of the webpage that linked to the resource is included with the request in an HTTP header called the \"referer\". This data captures 22 million `(referer, request)` pairs from a total of 4 billion requests collected during the month of January 2015. \n",
      "\n",
      "The dataset only includes requests for articles in the [main namespace](https://en.wikipedia.org/wiki/Wikipedia:Namespace) of the desktop version of English Wikipedia.\n",
      "\n",
      "Referers were [mapped]( https://github.com/ewulczyn/wmf/blob/master/mc/oozie/hive_query.sql#L30-L48) to a fixed set of values corresponding to internal traffic or external traffic from one of the top 5 global traffic sources to English Wikipedia, based on this scheme:\n",
      "    - an article in the main namespace of English Wikipedia -> the article title\n",
      "    - any Wikipedia page that is not in the main namespace of English Wikipedia -> `other-wikipedia`\n",
      "    - an empty referer -> `other-empty`\n",
      "    - a page from any other Wikimedia project -> `other-internal`\n",
      "    - Google -> `other-google`\n",
      "    - Yahoo -> `other-yahoo`\n",
      "    - Bing -> `other-bing`\n",
      "    - Facebook -> `other-facebook`\n",
      "    - Twitter -> `other-twitter`\n",
      "    - anything else -> `other`\n",
      "\n",
      " \n",
      "MediaWiki Redirects are used to forward clients from one page name to another. They can be useful if a particular article is referred to by multiple names, or has alternative punctuation, capitalization or spellings. Requests for pages that get redirected where mapped to the page they redirect to. For example, requests for 'Obama' redirect to the 'Barack_Obama' page. Redirects where resolved using a snapshot of the redirects table from Feb 20 2015.\n",
      "\n",
      "Redlinks are are links to an article that does not exist. Either the article was deleted after the creation of the link or the author intended to signal the need for such an article. Requests for redlinks are included in the data. \n",
      "\n",
      "We attempt to exclude spider traffic by classifying user agents with the [ua-parser](https://github.com/tobie/ua-parser) library and a few additonal Wikipedia specific filters. Furthermore, we attempt to filter out traffic from bots that request a page and then request all or most of the links on that page (BFS traversal) by setting a threshold on the rate at which a client can requests articles with the same referer. Requests that where made at too high of a rate get discarded. For the exact details, see [here](missing link). The threshold is quite high to avoid excluding human readers who open tabs as they read. As a result requests from slow moving bots are likely to remain in the data. More sophisticated bot detection, that evaluates the clients entire request graph is an avenue of future work.\n",
      "\n",
      "Finally, any `(referer, request)` pair with 10 or fewer observations was removed from the dataset. \n",
      "\n",
      "\n",
      "### Format\n",
      "The data includes the following 6 fields:\n",
      "\n",
      "- **prev_id:** if the referer does not correspond to an article in the main namespace of English Wikipedia, this value will be empty. Otherwise, it contains the unique MediaWiki page ID of the article corresponding to the referer i.e. the previous article the client was on\n",
      "- **curr_id:** the MediaWiki unique page ID of the article the client requested \n",
      "- **n:** the number of occurrences of the `(referer, request)` pair\n",
      "- **prev_title:** the result of mapping the referer URL to the fixed set of values described above\n",
      "- **curr_title:** the title of the article the client requested\n",
      "- **type** \n",
      "    - \"link\" if the referer and request are both articles and the referer links to the request\n",
      "    - \"redlink\" if the referer is an article and links to the request, but the request is not in the produiction enwiki.page table\n",
      "    - \"other\" if the referer and request are both articles but the referer does not link to the request. This can happen when clients search or spoof their refer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Getting to know the Data\n",
      "\n",
      "There are various quirks in the data due to the dynamic nature of the network of articles in English Wikipedia and the prevalence of requests automata. The following section gives a brief overview of the data fields and caveats that need to be kept in mind.\n",
      "\n",
      "### Top Referers\n",
      "First lets load the data into a pandas DataFrame and see who the top 10 referers to wikipedia are:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "df = pd.read_csv(\"../data/2015_01_clickstream_v5.tsv\", names =  ['prev_id', 'curr_id', 'n', 'prev', 'curr', 'type'], sep='\\t')\n",
      "df = df[['prev', 'curr', 'n', 'type']] # we won't use IDs here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.groupby('prev').sum().sort('n', ascending=False)[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The top referer by a large margin is Google. Next comes refererless traffic (usually clients using HTTPS). Then come other language Wikipedias and pages in English Wikipedia that are not in the main (i.e. article) namespace. Bing directs significanlty more traffic to Wikipedia than Yahoo. Social media referals are tiny compared to Google, with twitter leading to 10x more requests to Wikipedia than Facebook. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Trending on Social Media\n",
      "Lets look at what articles where trending on Twitter:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_twitter = df[df['prev'] == 'other-twitter']\n",
      "df_twitter.groupby('curr').sum().sort('n', ascending=False)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'm have no explanations for this, but if you find any of the tweets linking to these article, I would be curious to see why they got so many click throughs. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Most Requested Mising Pages\n",
      "Next lets look at the most popular redinks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_redlinks = df[df['type'] == 'redlink']\n",
      "df_redlinks.groupby('curr').sum().sort('n', ascending=False)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Four out of the five most popular redlinks refer to female celebrities. If you go to the 'Heather_Sossaman' article, you will see that the page was deleted on January 25. So it was not a really a redlink for most of January. Since the set of pages and links is constantly changing, the labeling of redlinks is not an exact science. In this case, we used the page and links tables from Feb 20th to mark a page as a relink."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Searching Within Wikipedia"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " Usually, clients navigate from one article to another through follwing a link. The other prominent case is search. The article from which the user searched is also passed as the referer to the found article. Hence, you will find a high count of `(Wikipedia, Chris_Kyle)` tuples. People went to the \"Wikipedia\" article to search for \"Chris_Kyle\". There is not a link to the \"Chris_Kyle\" article from the \"Wikipedia\" article. Finally, it is possible that the client messed with their referer header. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_search = df[df['type'] == 'other']\n",
      "df_search =  df_search[df_search.prev.str.match(\"^other.*\").apply(bool) == False]\n",
      "print \"Number of searches/ incorrect referers: %d\" % df_search.n.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_link = df[df['type'] == 'link']\n",
      "df_link =  df_link[df_link.prev.str.match(\"^other.*\").apply(bool) == False]\n",
      "print \"Number of links followed: %d\" % df_link.n.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Inflow vs Outflow"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You might be tempted to think that there can't be more traffic going out of a node than going into a node. This is not true for two reasons. People will follow links in multiple tabs as they read an article. Hence, a single pageview can lead to multiple records with that page as the referer. The data is also certain to include requests from bots which we did not correctly filter out. Bots will often follow most if not all the links in the article. Lets look at the ratio of incoming to outgoing links for the most requested pages."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_in = df.groupby('curr').sum()  # pageviews per article\n",
      "df_in.columns = ['in_count',]\n",
      "df_out = df.groupby('prev').sum() # link clicks per article\n",
      "df_out.columns = ['out_count',]\n",
      "df_in_out = df_in.join(df_out)\n",
      "df_in_out['ratio'] = df_in_out['out_count']/df_in_out['in_count']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_in_out.sort('in_count', ascending = False)[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking at the pages with the highest ratio of outgoing to incoming traffic reveals how messy the data is even after all the carefull data preparation described above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_in_out.sort('ratio', ascending = False)[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All of these pages have more traversals of a single link than they have requests for the page to begin with.  In other words, the client requested the page and then followed the same link over and over. As a post processing step, lets enforce that there can't be more traversals of a link than there where requests to the page."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_post = pd.merge(df, df_in, how='left', left_on='prev', right_index=True)\n",
      "df_post['n'] = df_post[['n', 'in_count']].min(axis=1)\n",
      "del df_post['in_count']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_in = df_post.groupby('curr').sum()  # pageviews per article\n",
      "df_in.columns = ['in_count',]\n",
      "df_out = df_post.groupby('prev').sum() # link clicks per article\n",
      "df_out.columns = ['out_count',]\n",
      "df_in_out = df_in.join(df_out)\n",
      "df_in_out['ratio'] = df_in_out['out_count']/df_in_out['in_count']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_in_out.sort('ratio', ascending = False)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_post.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# POC Network Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import snap"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CS = snap.TNEANet.New(len(set(df_post['curr'])), df_post.shape[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm = pd.merge(df_post, df_out, how='left', left_on='prev', right_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm = df_norm.sort(['prev', 'curr'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm['w'] = df_norm['n'] / df_norm['out_count']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm.head(40)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "node = \"Wikipedia\"\n",
      "n=10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prev = df[df['curr'] == node].sort(columns='n', ascending=False)[:n]\n",
      "tuples = [list(x) for x in prev.values]\n",
      "from pprint import pprint \n",
      "pprint(tuples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prev = df[df['prev'] == node].sort(columns='n', ascending=False)[:n]\n",
      "tuples = [list(x) for x in prev.values]\n",
      "from pprint import pprint \n",
      "pprint(tuples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}