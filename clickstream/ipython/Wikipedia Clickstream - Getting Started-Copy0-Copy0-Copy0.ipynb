{
 "metadata": {
  "name": "",
  "signature": "sha256:6c4901798ebe77dba9098dea4789c9c0982e9b0a570bc064c8aba170fa4f7ee9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Wikimedia Foundation (WMF) has publicly released hourly [pageviews](http://dumps.wikimedia.org/other/pagecounts-raw/) per article since 2007\n",
      "\n",
      "The Wikipedia request logs can give insight into how people across the world consume encyclopedic information. \n",
      "In raw form, this data is quite sensitive. Through heavy aggregation, however, it is possible to release data on how people are consuming knowledge in Wikipedia, without compromising individual privacy. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This post gives an introduction to working with the newly released [Wikipedia Clickstream](http://figshare.com/articles/Wikipedia_Clickstream/1305770) dataset. At a high level, it shows how people get to a Wikipedia article and what articles they click on next to learn more. Said another way, it gives a weighted network of articles, where each edge weight corresponds to how often people navigate from one page to another. To give an example, consider the figure below, which shows incoming and outgoing traffic to the \"London\" article."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(filename='London_Sankey.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some quick observations are that most people found the \"London\" page through Google Search and that only a small fraction of readers went on to another article. Before diving into some examples of working with the data, let me give a more detailed explanation of how the data was collected."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Data Preparation\n",
      "\n",
      "The data contains counts of `(referer, article)` pairs extracted from the request logs of English Wikipedia. When a client requests a resource by following a link or performing a search, the URI of the webpage that linked to the resource is included with the request in an HTTP header called the \"referer\". This data captures 22 million `(referer, article)` pairs from a total of 4 billion requests collected during the month of January 2015. \n",
      "\n",
      "Referers were [mapped]( https://github.com/ewulczyn/wmf/blob/master/mc/oozie/hive_query.sql#L30-L48) to a fixed set of values corresponding to internal traffic or external traffic from one of the top 5 global traffic sources to English Wikipedia, based on this scheme:\n",
      "    - an article in the main namespace of English Wikipedia -> the article title\n",
      "    - any Wikipedia page that is not in the main namespace of English Wikipedia -> `other-wikipedia`\n",
      "    - an empty referer -> `other-empty`\n",
      "    - a page from any other Wikimedia project -> `other-internal`\n",
      "    - Google -> `other-google`\n",
      "    - Yahoo -> `other-yahoo`\n",
      "    - Bing -> `other-bing`\n",
      "    - Facebook -> `other-facebook`\n",
      "    - Twitter -> `other-twitter`\n",
      "    - anything else -> `other`\n",
      "    \n",
      "    \n",
      "- Any `(referer, article)` pair with 10 or fewer observations was removed from the dataset. \n",
      "- The dataset only includes requests to articles in the [main namespace](https://en.wikipedia.org/wiki/Wikipedia:Namespace) of the desktop version of English Wikipedia\n",
      "- Requests to MediaWiki redirects and Redlinks are excluded\n",
      "- Spider traffic was excluded using the [ua-parser](https://github.com/tobie/ua-parser) library\n",
      "\n",
      "### Format\n",
      "The data includes the following 5 fields:\n",
      "\n",
      "- **prev_id:** if the referer does not correspond to an article in the main namespace of English Wikipedia, this value will be empty. Otherwise, it contains the unique MediaWiki page ID of the article corresponding to the referer i.e. the previous article the client was on\n",
      "- **curr_id:** the MediaWiki unique page ID of the article the client requested \n",
      "- **n:** the number of occurrences of the `(referer, article)` pair\n",
      "- **prev_title:** the result of mapping the referer URL to the fixed set of values described above\n",
      "- **curr_title:** the title of the article the client requested"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Getting Started\n",
      "\n",
      "First lets load the data into a pandas DataFrame and see who the top 10 referers to wikipedia are:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "df = pd.read_csv(\"2015_01_clickstream_v4.tsv\", names =  ['prev_id', 'curr_id', 'n', 'prev', 'curr', 'type'], sep='\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df[['prev', 'curr', 'n', 'type']] # we won't use IDs here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.groupby('prev').sum().sort('n', ascending=False)[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The top referer by a large margin is Google. Next comes refererless traffic (usually clients using HTTPS). Then come other language wikipedias and pages in english wikipedia that are not in the main (i.e. article) namespace. Social media referals are tiny compared to Google.  Its also interesting to note that Twitter sends 10x more people to Wikipedia than Facebook. Lets look at what articles where trending on Twitter:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_twitter = df[df['prev'] == 'other-twitter']\n",
      "df_twitter.groupby('curr').sum().sort('n', ascending=False)[:5]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'm have no explanations for this, but if you find any of the tweets linking to these article, I would be curious to see why they got so many click throughs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next lets look at the most popular redinks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_redlinks = df[df['type'] == 'redlink']\n",
      "df_redlinks.groupby('curr').sum().sort('n', ascending=False)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, lets dig deeper into how people navigate from one article to another. Usually, this navigation will happen through a user clicking on a link. The other case is search. The article from which the user searched is also passed as the referer to the found article. Hence, you will find a high count of `(Wikipedia, Chris_Kyle)` tuples. People went to the \"Wikipedia\" article to search for \"Chris_Kyle\". There is not link to the \"Chris_Kyle\" article from the \"Wikipedia\" article. It is possbile to remove these lines from the dataset by joining with the [enwiki.pagelinks](http://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia) table.\n",
      "\n",
      "\n",
      "You might be tempted to think that there can't be more traffic going out of a node than going into a node. This is not true for two reasons.\n",
      "\n",
      "1. Tabbed browsing\n",
      "\n",
      "People will follow links in multiple tabs as they read an article. Hence, a single pageview can lead to multiple records with that page as the referer.\n",
      "\n",
      "2. MediaWiki redirects\n",
      "\n",
      "When people want to see the article for Barack Obama, they might try to navigate directly to the\n",
      "\"Obama\" page. This page does not exists. Instead of throwing a 404, MediaWiki fetches content for the \"Barack_Obama\" page. This action leads to a line in the request logs for the \"Obama\" page only. This log line is not incorporated in this version of the data because \"Obama\" is not an actual page. To make things even more confusing,  when the client follows a link on the page they got by requesting \"Obama\", the refer will be \"Barack_Obama\". So if everybody gets to the \"Barack_Obama\" page via the \"Obama\" redirect, then \"Barack_Obama\" will have no pageviews but be listed as a refer whenver people followed a link. In the next release of this dataset we will map requests to redirects to the pages they redirect to."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_in = df.groupby('curr').sum()  # pageviews per article\n",
      "df_in.columns = ['in_count',]\n",
      "df_out = df.groupby('prev').sum() # link clicks per article\n",
      "df_out.columns = ['out_count',]\n",
      "df_in_out = df_in.join(df_out)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_in_out['ratio'] = df_in_out['out_count']/df_in_out['in_count']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_in_out.ix['Barack_Obama']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_in_out.sort('ratio', ascending = False)[:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "df_in_out['ratio'].hist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rows = []\n",
      "with open('2015_01_clickstream.tsv') as f:\n",
      "    for line in f:\n",
      "        row = line[:-1].split('\\t')\n",
      "        rows.append(row)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "node = \"Allan_Dwan\"\n",
      "n=10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prev = df[df['curr'] == node].sort(columns='n', ascending=False)[:n]\n",
      "tuples = [list(x) for x in prev.values]\n",
      "from pprint import pprint \n",
      "pprint(tuples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prev = df[df['prev'] == node].sort(columns='n', ascending=False)[:n]\n",
      "tuples = [list(x) for x in prev.values]\n",
      "from pprint import pprint \n",
      "pprint(tuples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}